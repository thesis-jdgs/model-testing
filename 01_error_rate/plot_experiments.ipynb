{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import attrs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "@attrs.define\n",
    "class RunReader:\n",
    "    path: str = attrs.field()\n",
    "    score_mean: float = attrs.field(init=False)\n",
    "    score_std: float = attrs.field(init=False)\n",
    "    name: str = attrs.field(init=False)\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        with open(\n",
    "                os.path.join(self.path, \"metrics/score_mean\"), \"r\"\n",
    "        ) as f:\n",
    "            self.score_mean = float(f.read().split(\" \")[1])\n",
    "        with open(\n",
    "                os.path.join(self.path, \"metrics/score_std\"), \"r\"\n",
    "        ) as f:\n",
    "            self.score_std = float(f.read().split(\" \")[1])\n",
    "        with open(\n",
    "                os.path.join(self.path, \"tags/model\"), \"r\"\n",
    "        ) as f:\n",
    "            self.name = f.read()\n",
    "\n",
    "    def dict(self):\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"score_mean\": self.score_mean,\n",
    "            \"score_std\": self.score_std,\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "{'name': 'SparseAdditiveBoostingRegressor',\n 'score_mean': 0.2199131782651799,\n 'score_std': 0.05131371566158607}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = RunReader(\n",
    "    \"mlruns/100940072706001416/3af9add646884a6b9ed3d690a6bed09a\"\n",
    ")\n",
    "r.dict()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "@attrs.define\n",
    "class ExperimentReader:\n",
    "    path: str = attrs.field()\n",
    "    runs: list[RunReader] = attrs.field(init=False)\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        dirs = [\n",
    "            d for d in os.listdir(self.path) if d != \"meta.yaml\"\n",
    "        ]\n",
    "        self.runs = [\n",
    "            RunReader(os.path.join(self.path, d)) for d in dirs\n",
    "        ]\n",
    "\n",
    "    def table(self):\n",
    "        df = pd.DataFrame(\n",
    "            [\n",
    "                run.dict() for run in self.runs\n",
    "            ]\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def means(self):\n",
    "        return self.table().groupby(\"name\").score_mean.max() #.rank(ascending=False)\n",
    "\n",
    "    def stds(self):\n",
    "        return self.table().groupby(\"name\").score_std.min() #.rank(ascending=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "name  DecisionTreeRegressor  ExplainableBoostingRegressor  \\\n0                  0.492598                      0.580912   \n1                  0.438221                      0.365574   \n2                  0.484158                      0.504628   \n3                  0.348067                      0.389941   \n4                  0.422491                      0.334714   \n..                      ...                           ...   \n117                0.435461                      0.616364   \n118                0.484984                      0.646965   \n119                0.544795                      0.729106   \n120                0.545170                      0.709915   \n121                0.337382                      0.599366   \n\nname  SparseAdditiveBoostingRegressor  XGBRegressor  randomforestregressor  \\\n0                            0.293820      0.621120               0.618507   \n1                            0.053777      0.647405               0.603019   \n2                            0.404053      0.703878               0.644872   \n3                           -0.029653      0.361401               0.470077   \n4                            0.053209      0.613659               0.571743   \n..                                ...           ...                    ...   \n117                          0.521694      0.558074               0.593470   \n118                          0.282799      0.629755               0.615132   \n119                          0.361839      0.721147               0.690359   \n120                          0.108542      0.727677               0.695060   \n121                          0.045015      0.426538               0.485580   \n\nname   ridgecv  \n0     0.063680  \n1     0.036903  \n2     0.095868  \n3     0.389633  \n4     0.071548  \n..         ...  \n117   0.627920  \n118   0.132180  \n119   0.131432  \n120   0.067148  \n121   0.071688  \n\n[122 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>name</th>\n      <th>DecisionTreeRegressor</th>\n      <th>ExplainableBoostingRegressor</th>\n      <th>SparseAdditiveBoostingRegressor</th>\n      <th>XGBRegressor</th>\n      <th>randomforestregressor</th>\n      <th>ridgecv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.492598</td>\n      <td>0.580912</td>\n      <td>0.293820</td>\n      <td>0.621120</td>\n      <td>0.618507</td>\n      <td>0.063680</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.438221</td>\n      <td>0.365574</td>\n      <td>0.053777</td>\n      <td>0.647405</td>\n      <td>0.603019</td>\n      <td>0.036903</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.484158</td>\n      <td>0.504628</td>\n      <td>0.404053</td>\n      <td>0.703878</td>\n      <td>0.644872</td>\n      <td>0.095868</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.348067</td>\n      <td>0.389941</td>\n      <td>-0.029653</td>\n      <td>0.361401</td>\n      <td>0.470077</td>\n      <td>0.389633</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.422491</td>\n      <td>0.334714</td>\n      <td>0.053209</td>\n      <td>0.613659</td>\n      <td>0.571743</td>\n      <td>0.071548</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>0.435461</td>\n      <td>0.616364</td>\n      <td>0.521694</td>\n      <td>0.558074</td>\n      <td>0.593470</td>\n      <td>0.627920</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>0.484984</td>\n      <td>0.646965</td>\n      <td>0.282799</td>\n      <td>0.629755</td>\n      <td>0.615132</td>\n      <td>0.132180</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>0.544795</td>\n      <td>0.729106</td>\n      <td>0.361839</td>\n      <td>0.721147</td>\n      <td>0.690359</td>\n      <td>0.131432</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>0.545170</td>\n      <td>0.709915</td>\n      <td>0.108542</td>\n      <td>0.727677</td>\n      <td>0.695060</td>\n      <td>0.067148</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td>0.337382</td>\n      <td>0.599366</td>\n      <td>0.045015</td>\n      <td>0.426538</td>\n      <td>0.485580</td>\n      <td>0.071688</td>\n    </tr>\n  </tbody>\n</table>\n<p>122 rows Ã— 6 columns</p>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs = os.listdir(\"mlruns\")[2:-1]\n",
    "df_list = []\n",
    "for dir_ in dirs:\n",
    "    experiment = ExperimentReader(os.path.join(\"mlruns\", dir_))\n",
    "    mean_ranking = experiment.means()\n",
    "    df_list.append(mean_ranking)\n",
    "merged_df = pd.concat(df_list, axis=1).T.reset_index(drop=True)\n",
    "merged_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "108   -0.600380\n90    -0.553327\n55    -0.548953\n93    -0.401378\n105   -0.242656\n         ...   \n48     0.761979\n70     0.881272\n39          NaN\n44          NaN\n65          NaN\nName: SparseAdditiveBoostingRegressor, Length: 122, dtype: float64"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[\"SparseAdditiveBoostingRegressor\"].sort_values()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "name  DecisionTreeRegressor  ExplainableBoostingRegressor  \\\n105                0.930462                      0.662913   \n79                 0.749382                      0.814054   \n7                  0.698090                      0.778966   \n33                 0.519755                      0.730648   \n54                 0.517703                      0.613331   \n49                 0.499762                      0.341560   \n120                0.545170                      0.709915   \n23                 0.989932                      0.958052   \n110                0.706012                      0.481989   \n19                 0.557997                      0.735173   \n101                0.598032                      0.790202   \n8                  0.384178                      0.700711   \n113                0.599688                      0.772346   \n84                 0.707279                      0.781496   \n18                 0.529474                      0.622983   \n63                 0.505594                      0.700926   \n50                 0.495399                      0.516102   \n74                 0.477699                      0.740085   \n104                0.571813                      0.756652   \n88                 0.670947                      0.672598   \n95                 0.614116                      0.787943   \n102                0.500450                      0.656555   \n37                 0.498854                      0.631546   \n22                 0.560006                      0.763620   \n119                0.544795                      0.729106   \n2                  0.484158                      0.504628   \n16                 0.813034                      0.803731   \n42                 0.707279                      0.781496   \n107                0.489316                      0.506011   \n77                 0.469567                      0.744394   \n35                 0.392793                      0.701821   \n27                 0.615122                      0.785370   \n26                 0.622894                      0.689632   \n66                 0.756934                      0.629802   \n96                 0.796546                      0.795857   \n48                 0.749382                      0.814054   \n70                 0.872293                      0.630257   \n44                 0.790610                      0.294008   \n\nname  SparseAdditiveBoostingRegressor  XGBRegressor  randomforestregressor  \\\n105                         -0.242656      0.916410               0.941184   \n79                          -0.078339      0.827954               0.819547   \n7                           -0.000150      0.774709               0.083430   \n33                           0.036162      0.686917               0.658520   \n54                           0.053747      0.700708               0.663116   \n49                           0.073262      0.700811               0.652509   \n120                          0.108542      0.727677               0.695060   \n23                           0.115376      0.988259               0.990098   \n110                          0.141747      0.666947               0.700389   \n19                           0.157652      0.729833               0.695308   \n101                          0.197839      0.750074               0.733732   \n8                            0.203201      0.640480               0.589646   \n113                          0.234228      0.740741               0.726621   \n84                           0.238467      0.797230               0.791282   \n18                           0.263649      0.718331               0.675230   \n63                           0.282832      0.702012               0.675928   \n50                           0.291266      0.703774               0.657863   \n74                           0.300608      0.681324               0.661770   \n104                          0.313922      0.716885               0.686762   \n88                           0.323845      0.731815               0.727854   \n95                           0.324129      0.774388               0.748663   \n102                          0.330216      0.723662               0.670317   \n37                           0.338529      0.708598               0.653688   \n22                           0.339010      0.741743               0.704119   \n119                          0.361839      0.721147               0.690359   \n2                            0.404053      0.703878               0.644872   \n16                           0.413799      0.813030               0.836733   \n42                           0.416292      0.797230               0.791282   \n107                          0.419883      0.701380               0.648840   \n77                           0.473304      0.687973               0.643784   \n35                           0.476067      0.615403               0.593425   \n27                           0.496817      0.768166               0.742124   \n26                           0.514995      0.752161               0.743060   \n66                           0.521694      0.740567               0.758952   \n96                           0.551844      0.742055               0.730976   \n48                           0.761979      0.827954               0.819547   \n70                           0.881272      0.901795               0.928438   \n44                                NaN      0.849951               0.844877   \n\nname   ridgecv  \n105   0.080980  \n79    0.370730  \n7     0.466857  \n33    0.056989  \n54    0.054118  \n49    0.066231  \n120   0.067148  \n23    0.599386  \n110  -0.097291  \n19    0.082199  \n101   0.096801  \n8     0.486619  \n113   0.077550  \n84    0.359278  \n18    0.104611  \n63    0.105416  \n50    0.050604  \n74    0.105290  \n104   0.101879  \n88    0.666003  \n95    0.498473  \n102   0.143215  \n37    0.096168  \n22    0.121855  \n119   0.131432  \n2     0.095868  \n16    0.820807  \n42    0.359278  \n107   0.100318  \n77    0.498015  \n35    0.508978  \n27    0.139823  \n26    0.281628  \n66    0.347154  \n96    0.751166  \n48    0.370730  \n70    0.914385  \n44    0.921774  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>name</th>\n      <th>DecisionTreeRegressor</th>\n      <th>ExplainableBoostingRegressor</th>\n      <th>SparseAdditiveBoostingRegressor</th>\n      <th>XGBRegressor</th>\n      <th>randomforestregressor</th>\n      <th>ridgecv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>105</th>\n      <td>0.930462</td>\n      <td>0.662913</td>\n      <td>-0.242656</td>\n      <td>0.916410</td>\n      <td>0.941184</td>\n      <td>0.080980</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>0.749382</td>\n      <td>0.814054</td>\n      <td>-0.078339</td>\n      <td>0.827954</td>\n      <td>0.819547</td>\n      <td>0.370730</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.698090</td>\n      <td>0.778966</td>\n      <td>-0.000150</td>\n      <td>0.774709</td>\n      <td>0.083430</td>\n      <td>0.466857</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.519755</td>\n      <td>0.730648</td>\n      <td>0.036162</td>\n      <td>0.686917</td>\n      <td>0.658520</td>\n      <td>0.056989</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>0.517703</td>\n      <td>0.613331</td>\n      <td>0.053747</td>\n      <td>0.700708</td>\n      <td>0.663116</td>\n      <td>0.054118</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>0.499762</td>\n      <td>0.341560</td>\n      <td>0.073262</td>\n      <td>0.700811</td>\n      <td>0.652509</td>\n      <td>0.066231</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>0.545170</td>\n      <td>0.709915</td>\n      <td>0.108542</td>\n      <td>0.727677</td>\n      <td>0.695060</td>\n      <td>0.067148</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.989932</td>\n      <td>0.958052</td>\n      <td>0.115376</td>\n      <td>0.988259</td>\n      <td>0.990098</td>\n      <td>0.599386</td>\n    </tr>\n    <tr>\n      <th>110</th>\n      <td>0.706012</td>\n      <td>0.481989</td>\n      <td>0.141747</td>\n      <td>0.666947</td>\n      <td>0.700389</td>\n      <td>-0.097291</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.557997</td>\n      <td>0.735173</td>\n      <td>0.157652</td>\n      <td>0.729833</td>\n      <td>0.695308</td>\n      <td>0.082199</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>0.598032</td>\n      <td>0.790202</td>\n      <td>0.197839</td>\n      <td>0.750074</td>\n      <td>0.733732</td>\n      <td>0.096801</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.384178</td>\n      <td>0.700711</td>\n      <td>0.203201</td>\n      <td>0.640480</td>\n      <td>0.589646</td>\n      <td>0.486619</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>0.599688</td>\n      <td>0.772346</td>\n      <td>0.234228</td>\n      <td>0.740741</td>\n      <td>0.726621</td>\n      <td>0.077550</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>0.707279</td>\n      <td>0.781496</td>\n      <td>0.238467</td>\n      <td>0.797230</td>\n      <td>0.791282</td>\n      <td>0.359278</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.529474</td>\n      <td>0.622983</td>\n      <td>0.263649</td>\n      <td>0.718331</td>\n      <td>0.675230</td>\n      <td>0.104611</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>0.505594</td>\n      <td>0.700926</td>\n      <td>0.282832</td>\n      <td>0.702012</td>\n      <td>0.675928</td>\n      <td>0.105416</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>0.495399</td>\n      <td>0.516102</td>\n      <td>0.291266</td>\n      <td>0.703774</td>\n      <td>0.657863</td>\n      <td>0.050604</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>0.477699</td>\n      <td>0.740085</td>\n      <td>0.300608</td>\n      <td>0.681324</td>\n      <td>0.661770</td>\n      <td>0.105290</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>0.571813</td>\n      <td>0.756652</td>\n      <td>0.313922</td>\n      <td>0.716885</td>\n      <td>0.686762</td>\n      <td>0.101879</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>0.670947</td>\n      <td>0.672598</td>\n      <td>0.323845</td>\n      <td>0.731815</td>\n      <td>0.727854</td>\n      <td>0.666003</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>0.614116</td>\n      <td>0.787943</td>\n      <td>0.324129</td>\n      <td>0.774388</td>\n      <td>0.748663</td>\n      <td>0.498473</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>0.500450</td>\n      <td>0.656555</td>\n      <td>0.330216</td>\n      <td>0.723662</td>\n      <td>0.670317</td>\n      <td>0.143215</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.498854</td>\n      <td>0.631546</td>\n      <td>0.338529</td>\n      <td>0.708598</td>\n      <td>0.653688</td>\n      <td>0.096168</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.560006</td>\n      <td>0.763620</td>\n      <td>0.339010</td>\n      <td>0.741743</td>\n      <td>0.704119</td>\n      <td>0.121855</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>0.544795</td>\n      <td>0.729106</td>\n      <td>0.361839</td>\n      <td>0.721147</td>\n      <td>0.690359</td>\n      <td>0.131432</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.484158</td>\n      <td>0.504628</td>\n      <td>0.404053</td>\n      <td>0.703878</td>\n      <td>0.644872</td>\n      <td>0.095868</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.813034</td>\n      <td>0.803731</td>\n      <td>0.413799</td>\n      <td>0.813030</td>\n      <td>0.836733</td>\n      <td>0.820807</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>0.707279</td>\n      <td>0.781496</td>\n      <td>0.416292</td>\n      <td>0.797230</td>\n      <td>0.791282</td>\n      <td>0.359278</td>\n    </tr>\n    <tr>\n      <th>107</th>\n      <td>0.489316</td>\n      <td>0.506011</td>\n      <td>0.419883</td>\n      <td>0.701380</td>\n      <td>0.648840</td>\n      <td>0.100318</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>0.469567</td>\n      <td>0.744394</td>\n      <td>0.473304</td>\n      <td>0.687973</td>\n      <td>0.643784</td>\n      <td>0.498015</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.392793</td>\n      <td>0.701821</td>\n      <td>0.476067</td>\n      <td>0.615403</td>\n      <td>0.593425</td>\n      <td>0.508978</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.615122</td>\n      <td>0.785370</td>\n      <td>0.496817</td>\n      <td>0.768166</td>\n      <td>0.742124</td>\n      <td>0.139823</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.622894</td>\n      <td>0.689632</td>\n      <td>0.514995</td>\n      <td>0.752161</td>\n      <td>0.743060</td>\n      <td>0.281628</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>0.756934</td>\n      <td>0.629802</td>\n      <td>0.521694</td>\n      <td>0.740567</td>\n      <td>0.758952</td>\n      <td>0.347154</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>0.796546</td>\n      <td>0.795857</td>\n      <td>0.551844</td>\n      <td>0.742055</td>\n      <td>0.730976</td>\n      <td>0.751166</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>0.749382</td>\n      <td>0.814054</td>\n      <td>0.761979</td>\n      <td>0.827954</td>\n      <td>0.819547</td>\n      <td>0.370730</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>0.872293</td>\n      <td>0.630257</td>\n      <td>0.881272</td>\n      <td>0.901795</td>\n      <td>0.928438</td>\n      <td>0.914385</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>0.790610</td>\n      <td>0.294008</td>\n      <td>NaN</td>\n      <td>0.849951</td>\n      <td>0.844877</td>\n      <td>0.921774</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.7\n",
    "solved = merged_df.max(axis=1) > threshold\n",
    "merged_df[solved].sort_values(by=\"SparseAdditiveBoostingRegressor\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "Index([  2,   7,   8,  16,  18,  19,  22,  23,  26,  27,  33,  35,  37,  42,\n        44,  48,  49,  50,  54,  63,  66,  70,  74,  77,  79,  84,  88,  95,\n        96, 101, 102, 104, 105, 107, 110, 113, 119, 120],\n      dtype='int64')"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.index[solved]"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
